# 基于numpy实现的机器学习算法


## 监督学习  

以下算法大多数使用场景主要为分类问题，部分算法基于回归可以实现预测行为。  

### 3.1 监督学习方面

### K近邻算法(KNN)  

该算法主要采用距离算法将未分类的测试数据与样本数据进行运算得出K个距离最近的数据，并根据少数服从
多数原则返回最终的分类结果，其中相关的实现代码可以参考如下。  

* [基于numpy的knn算法](knn.py)  

由于本算法的核心依赖距离算法，所以根据实际的使用场景选择适当的距离算法进行替换，当前可以利用的距离算法主要有：  

1. 欧式距离  
2. 马氏距离  
3. 曼哈顿距离  
4. 切比雪夫距离  
5. 闵可夫斯基距离  
6. 夹角余弦  
7. 汉明距离  
8. 杰卡德距离  

关于各类算法的介绍可以参考[本文章](https://www.cnblogs.com/soyo/p/6893551.html)

### 决策树  

该算法其实就是采用IF...THEN...ELSE形式进行组织，从而形成一个树结构。从人类本身的认知出发一个问题
往往会有多个选择，但是考虑计算机本身的特点以及效率等，往往会会采用二叉树。相关的算法可以通过如下文件
进行学习：  

* [Numpy原始算法](tree.py)  
* [随机森林](treePlotter.py)  

树回归（预测）相关源码  
* [Numpy原始算法](regTrees.py)  

以上已经实现的ID3与CART算法，至此还剩下C4.5算法，关于三种算法的具体原理介绍可以参考如下文章：  

1. [ID3算法](https://www.infoq.cn/article/ZXig7JMhPzeH97zM5l0z)  
该算法依赖[信息嫡](https://zh.wikipedia.org/wiki/%E7%86%B5_(%E4%BF%A1%E6%81%AF%E8%AE%BA))
与 
[信息增益](https://www.zhihu.com/question/22104055)
进行特征选择从而进行树的创建。  
2. [C4.5算法](https://blog.csdn.net/zhihua_oba/article/details/70632622)  
3. [CART算法](https://blog.csdn.net/ACdreamers/article/details/44664481)

### 朴素贝叶斯  

关于该算法的基本介绍见[本文](https://zhuanlan.zhihu.com/p/26262151) 下面我们将主要介绍
其算法的实现：  

* [Numpy原始算法](bayes.py)  

### 逻辑回归（Logistic回归）  

首先在了解具体的逻辑回归算法前我们需要先了解几个相关的基础知识算法以便于我们更好的去
了解其源码的实现逻辑和最终应用场景的选择。  

* [sigmoid算法](https://www.jianshu.com/p/506595ec4b58)  

由于我们这里处理的是二分类问题，对于Logistic最终4计算的结果值，我们需要将其限定在一个实当的范围内
从而实现分类，其实就是计算概率，如果概率大于一半进行选择。  

* [梯度上升算法简单理解](https://www.jianshu.com/p/eb94c60015c7)  
* [梯度下降算法](https://www.cnblogs.com/sench/p/9817188.html)  

因为本身函数的特点，我们需要计算回归系数，这里我们主要采用了梯度上升算法进行计算，当然读者也可以
采用梯度下降算法。但是梯度上升算法在数据量较多时，由于其本身需要进行全数据的迭代所以我们还需要引
入一个更高效的算法，即随机梯度提升算法，可以仅用一个样本点更新回归系数。下面我们可以参考具体的实
现代码进行学习：  

* [基于sklearn的逻辑回归算法](logisticRegression.py)  

### 线性回归  

对于了解线性回归可以参考[本文章](https://zhuanlan.zhihu.com/p/53979679) 进行相关关系，其中还包含了关于局部加权线性回归方式，
针对常规的数据这是没有问题的，但是如果样本数据中的特征多余样本数据本身那么就存在问题了，此时我们就需要通过引入岭回归、lasso与前向
逐步回归算法。  

* [基于numpy的线性回归](lineRegression.py)  

### 元计算  

上面我们介绍了多个分类算法，为了得到最好的结果我们有时可能需要结合不同算法或相同算法不同配置进行组合，这就叫做元计算。本篇将主要介绍
同算法不同配置的情况，主要采用的是[adaboost算法](https://blog.csdn.net/px_528/article/details/72963977) ，对应的源码参考如下：  

* [numpy原始算法](adaboost.py)  

## 无监督学习  

### K均值聚类  

k均值聚类是最简单也最常用的聚类算法之一。它试图找到代表数据特定区域的簇中心。算法交替执行如下两个步骤：将每个数据点分配给最近的簇中心，
然后将每个簇中心设置为所分配的所有数据点的平均值。如果簇的分配不再发生变化，那么算法结束。以下将列举出对应实现的方式。  

* [Numpy原始算法](KMeans.py)  
