# sklearn机器学习

## 一、监督学习  

以下算法大多数使用场景主要为分类问题，部分算法基于回归可以实现预测行为。  

### 1.1 监督学习方面

### K近邻算法(KNN)  

该算法主要采用距离算法将未分类的测试数据与样本数据进行运算得出K个距离最近的数据，并根据少数服从
多数原则返回最终的分类结果，其中相关的实现代码可以参考如下。  

* [基于sklearn的knn算法](knn.py)  

由于本算法的核心依赖距离算法，所以根据实际的使用场景选择适当的距离算法进行替换，当前可以利用的距离算法主要有：  

1. 欧式距离  
2. 马氏距离  
3. 曼哈顿距离  
4. 切比雪夫距离  
5. 闵可夫斯基距离  
6. 夹角余弦  
7. 汉明距离  
8. 杰卡德距离  

关于各类算法的介绍可以参考[本文章](https://www.cnblogs.com/soyo/p/6893551.html)

### 决策树  

该算法其实就是采用IF...THEN...ELSE形式进行组织，从而形成一个树结构。从人类本身的认知出发一个问题
往往会有多个选择，但是考虑计算机本身的特点以及效率等，往往会会采用二叉树。相关的算法可以通过如下文件
进行学习：  

* [sklearn库使用](tree.py)  

以上已经实现的ID3与CART算法，至此还剩下C4.5算法，关于三种算法的具体原理介绍可以参考如下文章：  

1. [ID3算法](https://www.infoq.cn/article/ZXig7JMhPzeH97zM5l0z)  
该算法依赖[信息嫡](https://zh.wikipedia.org/wiki/%E7%86%B5_(%E4%BF%A1%E6%81%AF%E8%AE%BA))
与 
[信息增益](https://www.zhihu.com/question/22104055)
进行特征选择从而进行树的创建。  
2. [C4.5算法](https://blog.csdn.net/zhihua_oba/article/details/70632622)  
3. [CART算法](https://blog.csdn.net/ACdreamers/article/details/44664481)

### 朴素贝叶斯  

关于该算法的基本介绍见[本文](https://zhuanlan.zhihu.com/p/26262151) 下面我们将主要介绍
其算法的实现：  

* [sklearn库使用](bayes.py)  

### 逻辑回归（Logistic回归）  

首先在了解具体的逻辑回归算法前我们需要先了解几个相关的基础知识算法以便于我们更好的去
了解其源码的实现逻辑和最终应用场景的选择。  

* [sigmoid算法](https://www.jianshu.com/p/506595ec4b58)  

由于我们这里处理的是二分类问题，对于Logistic最终4计算的结果值，我们需要将其限定在一个实当的范围内
从而实现分类，其实就是计算概率，如果概率大于一半进行选择。  

* [梯度上升算法简单理解](https://www.jianshu.com/p/eb94c60015c7)  
* [梯度下降算法](https://www.cnblogs.com/sench/p/9817188.html)  

因为本身函数的特点，我们需要计算回归系数，这里我们主要采用了梯度上升算法进行计算，当然读者也可以
采用梯度下降算法。但是梯度上升算法在数据量较多时，由于其本身需要进行全数据的迭代所以我们还需要引
入一个更高效的算法，即随机梯度提升算法，可以仅用一个样本点更新回归系数。下面我们可以参考具体的实
现代码进行学习：  

* [基于numpy的逻辑回归算法](logisticRegression.py)  

### 支持向量机（SVM）  

关于该算法比较好的解释可以参考[SVM原理](https://www.cnblogs.com/LeftNotEasy/archive/2011/05/02/basic-of-svm.html) 
文章。从个人简短的理解来说，该算法在面对非线性问题下，将采用超平面，即多维空间进行数据分类的切分。以下这个视频将可以较形式的展示这一
过程[演示视频](https://v.qq.com/x/page/k05170ntgzc.html)  

为了训练模型我们此时需要引入[SMO算法](https://www.jianshu.com/p/eef51f939ace) （序列最小优化算法）来解决二次规划问题，当然
如果读者并不想过多接触具体的核心算法逻辑，可以参考具体的实现源码进行学习应用：  

* [支持向量机](SVM.py)  

### 线性回归  

对于了解线性回归可以参考[本文章](https://zhuanlan.zhihu.com/p/53979679) 进行相关关系，其中还包含了关于局部加权线性回归方式，
针对常规的数据这是没有问题的，但是如果样本数据中的特征多余样本数据本身那么就存在问题了，此时我们就需要通过引入岭回归、lasso与前向
逐步回归算法。  

* [基于sklearn的线性回归](lineRegression.py)  

### 神经网络  

这里我们以入门的MLP（多层感知机）为例，相关的代码可以参考：  

* [基于sklearn的MLP使用](MLP.py)  

### K均值聚类  

k均值聚类是最简单也最常用的聚类算法之一。它试图找到代表数据特定区域的簇中心。算法交替执行如下两个步骤：将每个数据点分配给最近的簇中心，
然后将每个簇中心设置为所分配的所有数据点的平均值。如果簇的分配不再发生变化，那么算法结束。以下将列举出对应实现的方式。  

* [Sklearn算法](KMeanss.py)  

### 凝聚聚类  

该算法指的是许多基于相同原则构建的聚类算法，这一原则是：算法首先声明每个点是自己的簇，然后合并两个最相似的簇，直到满足某种停止准则为止。
其中sk库的停止准则是簇的个数，因此相似的簇被合并，直到仅剩下指定个数的簇。其中库实现了以下三种选择。  

* ward：默认选项。ward挑选两个簇来合并，使得所有簇中的方差增加最小。这通常会得到大小差不多相等的簇。  
* average：该链接将簇中所有点之间平均距离最小的两个簇合并。  
* complete：该链接将簇中点之间最大距离最小的两个簇合并。  

ward适用于大多数数据集，如果簇中的成员个数非常不同，那么average或complete可能效果更好。以下为具体的算法代码：  

* [Sklearn算法](cluster/agglomerative.py)  

### 密度聚类

该算法名叫DBSCAN（具有噪声的基于密度的空间聚类应用），该算法主要优点在于不需要用户先验地设置簇的个数，可以划分具有复杂形状的簇，还可以
找出不属于任何簇的点。DBSCAN比凝聚聚类和K均值稍慢，但仍可以扩展到相对较大的数据集。  

它主要的参数需要min_samples和eps。算法首先任意选取一个点，然后找到这个点的距离小于eps的所有的点。如果距起始点的距离在eps之内的数据点
个数小于min_samples，那么这个点被标记为噪声，也就是说它不属于任何簇。如果距离在eps之内的数据点个数大于min_samples，则这个点被标记为
核心样本，并被分配一个新的簇标签。然后访问该点的所有邻居，如果它们没有被分配一个簇，那么就分配刚刚创建的新的簇标签。如果它们是核心样本，
那么就以此访问其邻居，以此类推。  

* [Sklearn算法](cluster/Dbscan.py)  

### 聚类算法对比评估  

为了论证不同聚类算法的性能，为此我们需要利用一些评估方式。其中可用于聚类算法相对于真实聚类的结果，其中最重要的是调整rand指数（ARI）
和归一化互信息（NMI），二者都给出了定量的度量，其最佳值为1，0标识不相关的聚类。  

以上两种评估方式存在一个很大的问题就是需要真实值来比较结果。但是实际情况可能并没有真实值进行评估，此时我们就需要利用轮廓系数。但它们
在实践中的效果并不好。轮廓分数计算一个簇的紧致度，其值越大越好，最高分数为1。  

* [Sklearn算法](cluster/assessment.py)  
