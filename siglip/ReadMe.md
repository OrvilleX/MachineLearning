# SigLIP å›¾æ–‡å¯¹ç…§æ¨¡å‹

CLIPæ¨¡å‹æ˜¯ä½¿ç”¨softmaxï¼ˆäº¤å‰ç†µæŸå¤±ï¼‰è®­ç»ƒï¼Œå…¶ä¸­softmaxéœ€è¦å¯¹æ‰€æœ‰æˆå¯¹ç›¸ä¼¼æ€§æœ‰ä¸€ä¸ªå…¨å±€è§†å›¾ï¼Œä»¥ä¾¿å½’ä¸€åŒ–åˆ°æ¦‚ç‡ã€‚ç»ç ”ç©¶å‘ç°ï¼Œå¯ç”¨æ›´ç®€å•çš„ sigmoid æŸå¤±æ¥ä»£æ›¿å®ƒï¼Œå…¶ä¸éœ€è¦è¿™ç§å…¨å±€è§†å›¾ã€‚sigmoid æŸå¤±åŒæ—¶å…è®¸åœ¨é¢„è®­ç»ƒæœŸé—´è¿›ä¸€æ­¥æ‰©å¤§æ‰¹é‡å¤§å°ï¼Œåœ¨è¾ƒå°çš„æ‰¹é‡å°ºå¯¸ä¸‹ä¹Ÿè¡¨ç°å¾—æ›´å¥½ã€‚è¯¥æ¨¡å‹åœ¨é›¶æ ·æœ¬å›¾åƒåˆ†ç±»å’Œå›¾åƒæ–‡æœ¬æ£€ç´¢æ–¹é¢éƒ½ä¼˜äºCLIPæ¨¡å‹ï¼Œæ¯”å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š
![](/media/17378753829365.jpg)

å‚è€ƒæ–‡æ¡£
* [SigLIPæ¨¡å‹çš„æ¨ç†demo](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/SigLIP/Inference_with_(multilingual)_SigLIP%2C_a_better_CLIP_model.ipynb)  
* [SigLIPå®˜æ–¹ä½¿ç”¨æ–‡æ¡£](https://huggingface.co/docs/transformers/main/model_doc/siglip#)  
* [siglip-so400m-patch14-384ç›®å‰æœ€ä¼˜æ¨¡å‹](https://huggingface.co/google/siglip-so400m-patch14-384)  
* [æ–‡æ¡£ä¸­ç›¸å…³ä»£ç ä»“åº“](https://github.com/OrvilleX/Apollo-LLM-Start/tree/main/siglip)

æ¨ç†ç³»ç»Ÿç¯å¢ƒ
* CPU: 12 vCPU Intel(R) Xeon(R) Platinum 8352V CPU @ 2.10GHz
* MEM: 90GB
* GPU: RTX 4090(24GB) * 1
* ç³»ç»Ÿ: ubuntu22.04
* ç¯å¢ƒ: PyTorch  2.5.1 + Python 3.12 + Cuda  12.4

## ä¸€ã€å¿«é€Ÿä½¿ç”¨

### 1. åŸºæœ¬é…ç½®

```bash
# å°†pipè·Ÿcondaä¾èµ–åŒ…å®‰è£…åˆ°æ•°æ®ç›˜
mkdir -p /root/autodl-tmp/conda/pkgs
conda config --add pkgs_dirs /root/autodl-tmp/conda/pkgs

mkdir -p /root/autodl-tmp/conda/envs
conda config --add envs_dirs /root/autodl-tmp/conda/envs
```

```bash
# åˆ›å»ºcondaç¯å¢ƒ
conda create -y -n siglip python=3.10 -q
conda init bash && source /root/.bashrc

# å®‰è£…ä¾èµ–åº“
conda activate siglip
pip install torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 --index-url https://download.pytorch.org/whl/cu124

# é…ç½®åˆ°jupyterlab
conda install ipykernel
ipython kernel install --user --name=siglip
```

### 2. é¡¹ç›®é…ç½®

```bash
# å®‰è£…git lfsä¸‹è½½é•œåƒ
sudo apt update
sudo apt install nload git-lfs -y

# å®‰è£…é¡¹ç›®ä¾èµ–
conda activate siglip
pip install transformers==4.44.0 sentencepiece protobuf jupyterlab_widgets

# ç¦»çº¿ä¸‹è½½æ¨¡å‹
git lfs install
git clone https://huggingface.co/google/siglip-so400m-patch14-384
```

### 3. å¿«é€Ÿåº”ç”¨

æ¥ä¸‹æ¥å°†åŸºäºJupyter labè¿›è¡ŒåŸºæœ¬ä½¿ç”¨çš„æ–¹å¼ä»‹ç»ï¼Œé¦–å…ˆæ˜¯åŠ è½½ç›®å‰æœ€å¥½çš„æ¨¡å‹ï¼Œå®ƒå…·æœ‰
shape-optimizedï¼ˆsoï¼‰æ¶æ„ï¼Œè¯¥æ¨¡å‹çš„æ€§èƒ½æ˜æ˜¾ä¼˜äº ViT å·¨å‹æ¶æ„ï¼ŒåŒæ—¶ä½“ç§¯è¦å°å¾—å¤šã€‚

```python
from transformers import AutoProcessor, AutoModel

# ä¸‹æ–¹ç¦»çº¿æ¨¡å‹çš„è·¯å¾„éœ€è¦æ ¹æ®å®é™…çš„è·¯åŸºè¿›è¡Œè°ƒæ•´
processor = AutoProcessor.from_pretrained("/root/autodl-tmp/siglip-so400m-patch14-384")
model = AutoModel.from_pretrained("/root/autodl-tmp/siglip-so400m-patch14-384")
```

å‡†å¤‡éœ€è¦è¯†åˆ«å›¾ç‰‡ï¼Œé€šè¿‡ç½‘ç»œè¿›è¡Œä¸‹è½½å¹¶åŠ è½½ã€‚

```python
from PIL import Image
import requests

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)
image
```

ä¸‹é¢å‡†å¤‡å¯¹åº”çš„éœ€è¦è¿›è¡Œæµ‹è¯•éªŒè¯çš„æ–‡å­—ä¸å›¾ç‰‡è¿›è¡Œå¤„ç†ã€‚

```python
texts = ["a photo of 2 cats", "a photo of 2 hamburgers", "a photo of 2 dogs"]

inputs = processor(text=texts, images=image, padding="max_length", return_tensors="pt")
for k,v in inputs.items():
  print(k,v.shape)
```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æ‰§è¡Œå‰å‘ä¼ é€’å¹¶è·å–æ¯ä¸ªå›¾åƒ-æ–‡æœ¬å¯¹çš„æœªè§„èŒƒåŒ–åˆ†æ•°ï¼Œåœ¨åº”ç”¨sigmoidæ¿€æ´»å‡½æ•°ï¼Œè½¬æ¢ä¸ºå•ä¸ªæ¦‚ç‡ã€‚

```python
import torch

with torch.no_grad():
  outputs = model(**inputs)

logits_per_image = outputs.logits_per_image
probs = torch.sigmoid(logits_per_image) # these are the probabilities
print(f"{probs[0][0]:.1%} that image 0 is '{texts[0]}'")
print(f"{probs[0][1]:.1%} that image 0 is '{texts[1]}'")
print(f"{probs[0][2]:.1%} that image 0 is '{texts[2]}'")
```

ä¸Šè¿°çš„æµç¨‹åŒ…å«äº†è¾ƒå¤šçš„å‰å¤„ç†ä¸åå¤„ç†ï¼Œä¸ºäº†æ›´ç®€ä¾¿çš„è¿›è¡Œåˆ†æå¤„ç†ï¼Œå¯ä»¥é€šè¿‡ç®¡é“(pipelines)ç®€åŒ–è¿™ä¸€æ­¥éª¤ã€‚

```python
from transformers import pipeline
from PIL import Image
import requests
import torch

device = 0 if torch.cuda.is_available() else -1
# load pipe
image_classifier = pipeline(task="zero-shot-image-classification", model="/root/autodl-tmp/siglip-so400m-patch14-384", device=device)

# load image
url = 'http://images.cocodataset.org/val2017/000000039769.jpg'
image = Image.open(requests.get(url, stream=True).raw)

# inference
outputs = image_classifier(image, candidate_labels=["2 cats", "a plane", "a remote"])
outputs = [{"score": round(output["score"], 4), "label": output["label"] } for output in outputs]
print(outputs)
```

## äºŒã€è¿›é˜¶ä½¿ç”¨

### 1. é‡‡ç”¨Flash Attention 2.0

ä¸ºäº†èƒ½å¤Ÿä½¿ç”¨å…¶æ¡†æ¶è¿›è¡ŒåŠ é€Ÿï¼Œé¦–å…ˆéœ€è¦åœ¨å¯¹åº”çš„ç¯å¢ƒä¸­å®‰è£…å¯¹åº”çš„ä¾èµ–ã€‚  

```bash
conda activate siglip
pip install flash-attn==2.7.3 accelerate==1.3.0
```

å®‰è£…å®Œæˆä¸Šè¿°ä¾èµ–åï¼Œä½¿ç”¨Flash Attention 2.0é’ˆå¯¹æ³¨æ„åŠ›å¤´è¿›è¡ŒåŠ é€Ÿæ¨ç†ã€‚

```python
import torch
import requests
from PIL import Image
from transformers import SiglipProcessor, SiglipModel
device = "cuda"

model = SiglipModel.from_pretrained(
    "/root/autodl-tmp/siglip-so400m-patch14-384",
    attn_implementation="flash_attention_2",
    torch_dtype=torch.float16,
    device_map=device,
)
processor = SiglipProcessor.from_pretrained("/root/autodl-tmp/siglip-so400m-patch14-384")

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)

candidate_labels = ["2 cats", "2 dogs"]
texts = [f'This is a photo of {label}.' for label in candidate_labels]
inputs = processor(text=texts, images=image, padding="max_length", return_tensors="pt")
inputs.to(device)

with torch.no_grad():
    with torch.autocast(device, dtype=torch.float16):
        outputs = model(**inputs)

logits_per_image = outputs.logits_per_image
probs = torch.sigmoid(logits_per_image)
print(f"{probs[0][0]:.1%} that image 0 is '{candidate_labels[0]}'")
```

### 2. ä½¿ç”¨ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ› ï¼ˆSDPAï¼‰

PyTorchåŒ…å«ä¸€ä¸ªåŸç”Ÿç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ› ï¼ˆSDPAï¼‰ è¿ç®—ç¬¦ï¼Œä½œä¸º torch.nn.functional çš„ä¸€éƒ¨åˆ†ã€‚æ­¤åŠŸèƒ½ åŒ…å«å¤šç§å®ç°ï¼Œè¿™äº›å®ç°å¯ä»¥æ ¹æ® inputs å’Œä½¿ç”¨çš„ç¡¬ä»¶è¿›è¡Œåº”ç”¨ï¼Œè¿™é‡Œéœ€è¦torch>=2.1.1æ‰å¯ä»¥ä½¿ç”¨ã€‚

```python
# åŸºäºä¸Šè¿°ä¾‹å­åªè¦ä¿®æ”¹å…¶ä¸­éƒ¨åˆ†å³å¯
model = SiglipModel.from_pretrained(
    "/root/autodl-tmp/siglip-so400m-patch14-384",
    attn_implementation="sdpa",
    torch_dtype=torch.float16,
    device_map=device,
)
```

å…³äºä¸Šè¿°åŸºäºFlash Attention 2.0 ä¸ SDPA çš„çš„æ€§èƒ½å¯¹æ¯”å·®å¼‚å¦‚ä¸‹å›¾æ‰€ç¤º

![](/media/17380499894001.png)

## ä¸‰ã€å¾®è°ƒæ¨¡å‹

é’ˆå¯¹å¤šæ¨¡æ€çš„ä½¿ç”¨åœºæ™¯ä¸­ï¼Œé’ˆå¯¹ç°å®ä¸–ç•Œä¸­ä»¥åŠç‰¹å®šé¢†åŸŸåœºæ™¯çš„ä½¿ç”¨å¾€å¾€éœ€è¦è¡¥å……é’ˆå¯¹æ€§çš„å›¾ç‰‡-æ–‡å­—å¯¹å®ç°æ›´é«˜æ•ˆçš„æ¨ç†èƒ½åŠ›ï¼Œä¸ºæ­¤
ä¸‹è¿°å°†åŸºäºä¸Šè¿°ç›®å‰æœ€ä¼˜çš„æ¨¡å‹åŸºç¡€ä¸Šè¿›è¡Œå¾®è°ƒä»¥å®ç°ç‰¹å®šåœºæ™¯æ•°æ®çš„é€‚é…ã€‚ç”±äºæ¨ç†å¯¹äºæœºå™¨çš„è¦æ±‚ä¸é«˜ï¼Œä½†æ˜¯åœ¨è¿›è¡Œæ•°æ®è®­ç»ƒåˆ™éœ€è¦
æ»¡è¶³ä¸€å®šè¦æ±‚çš„æœºå™¨é…ç½®æ‰å¯ä»¥é¡ºåˆ©çš„è¿›è¡Œå¾®è°ƒæ¨ç†ï¼Œä¸ºæ­¤éœ€è¦æ»¡è¶³ä»¥ä¸‹æœ€ä½é…ç½®çš„è¦æ±‚ã€‚

å¾®è°ƒç³»ç»Ÿç¯å¢ƒ
* CPU: 6 vCPU Intel(R) Xeon(R) Gold 6130 CPU @ 2.10GHz
* MEM: 25GB
* GPU: V100-32GB(32GB)
* ç³»ç»Ÿ: ubuntu22.04
* ç¯å¢ƒ: PyTorch  2.3.0 + Python 3.12 +Cuda  12.1

### 1. æµ‹è¯•æ•°æ®

ç»§ä¸Šè¿°å·²åˆ›å»ºçš„siglipç¯å¢ƒï¼Œè¿˜éœ€è¦å®‰è£…ä»¥ä¸‹éœ€è¦çš„åº“ã€‚

```bash
# å®‰è£…é¢å¤–çš„åº“
pip install -q datasets

# ä¸‹è½½æœ¬æ¬¡ä½¿ç”¨çš„æ•°æ®é›†
curl -L -o ~/autodl-tmp/multi-label-image-classification-dataset.zip\
  https://www.kaggle.com/api/v1/datasets/download/meherunnesashraboni/multi-label-image-classification-dataset
unzip multi-label-image-classification-dataset.zip
```

### 2. å¿«é€Ÿè®­ç»ƒ

å°†ä¸‹è½½çš„csvè¯»å–ä¸ºPandasæ•°æ®é›†ï¼Œæ¯è¡Œéƒ½åŒ…å«ä¸€ä¸ªè®­ç»ƒç¤ºä¾‹ï¼Œå…¶ä¸­åŒ…å«å›¾åƒçš„æ–‡ä»¶åå’Œç›¸åº”çš„ one-hot ç¼–ç æ ‡ç­¾ã€‚
```python
import pandas as pd

df = pd.read_csv("/root/autodl-tmp/multilabel_modified/multilabel_classification(2).csv")
df.head()
```

åˆ›å»ºä¸€ä¸ª id2label å­—å…¸ï¼Œå°†æ•´æ•°æ˜ å°„åˆ°å­—ç¬¦ä¸²ã€‚
```python
labels = list(df.columns)[2:]
id2label = {id: label for id, label in enumerate(labels)}
print(id2label)
```

æ¥ä¸‹æ¥åŠ è½½ç¦»çº¿æ¨¡å‹ä¸å›¾åƒå¤„ç†å™¨ï¼Œå…¶ä¸­å°†problem_typeæŒ‡å®šä¸º â€œmulti_label_classificationâ€,å…¶æ˜¯å‘Šè¯‰æ¨¡å‹å½“å‰ä¸ºå¤šæ ‡ç­¾åˆ†ç±»ï¼Œä»è€Œä¿ƒä½¿å…¶ä½¿ç”¨æ­£ç¡®çš„æ¿€æ´»å‡½æ•°ï¼Œ
```python
from transformers import AutoImageProcessor, AutoModelForImageClassification
import torch

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model_id = "/root/autodl-tmp/siglip-so400m-patch14-384" # æŒ‡å®šä¸ºå®é™…æœ¬åœ°ç¦»çº¿æ¨¡å‹è·¯å¾„

processor = AutoImageProcessor.from_pretrained(model_id, device=device)
model = AutoModelForImageClassification.from_pretrained(model_id, problem_type="multi_label_classification", id2label=id2label)
model = model.to(device) 
```

åˆ›å»ºæ•°æ®é›†è¯»å–ç±»ï¼Œä»è€Œç¡®ä¿èƒ½å¤Ÿæ­£ç¡®çš„è¯»å–å›¾ç‰‡ä»¥åŠåˆ†ç±»æ ‡ç­¾å¹¶è½¬æ¢ä¸ºæ­£ç¡®çš„æ ¼å¼
```python
from torch.utils.data import Dataset
import torch
from PIL import Image
import os
import numpy as np

class MultiLabelDataset(Dataset):
  def __init__(self, root, df, transform):
    self.root = root
    self.df = df
    self.transform = transform

  def __getitem__(self, idx):
    item = self.df.iloc[idx]
    # get image
    image_path = os.path.join(self.root, item["Image_Name"])

    if not os.path.exists(image_path):
        return None

    image = Image.open(image_path).convert("RGB")

    # prepare image for the model
    pixel_values = self.transform(image)

    # get labels
    labels = item[2:].values.astype(np.float32)

    # turn into PyTorch tensor
    labels = torch.from_numpy(labels)

    return pixel_values, labels

  def __len__(self):
    return len(self.df)
```

ä¸ºäº†å‡†å¤‡çš„å›¾åƒï¼Œå°†ä½¿ç”¨ Torchvision åŒ…ï¼Œå®ƒæä¾›äº†è‹¥å¹²å›¾åƒè½¬æ¢å·¥å…·å°†å›¾åƒå¤§å°è°ƒæ•´ä¸ºæ¨¡å‹é¢„æœŸçš„å¤§å°ï¼ˆåœ¨æœ¬ä¾‹ä¸­ä¸º 384ï¼‰ï¼Œ
å¹¶ä¸”ä½¿ç”¨é€‚å½“çš„å¹³å‡å€¼å’Œæ ‡å‡†åå·®å¯¹é¢œè‰²é€šé“è¿›è¡Œæ ‡å‡†åŒ–ã€‚
```python
from torchvision.transforms import Compose, Resize, ToTensor, Normalize

# get appropriate size, mean and std based on the image processor
size = processor.size["height"]
mean = processor.image_mean
std = processor.image_std

transform = Compose([
    Resize((size, size)),
    ToTensor(),
    Normalize(mean=mean, std=std),
])

train_dataset = MultiLabelDataset(root="/root/autodl-tmp/multilabel_modified/images",
                                  df=df, transform=transform)
len(train_dataset)
```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºç›¸åº”çš„ PyTorch DataLoaderï¼Œä»¥è·å–æ‰¹é‡è®­ç»ƒç¤ºä¾‹ï¼ˆå› ä¸ºç¥ç»ç½‘ç»œé€šå¸¸ä½¿ç”¨éšæœºæ¢¯åº¦ä¸‹é™ = SGD å¯¹æ‰¹é‡æ•°æ®è¿›è¡Œè®­ç»ƒï¼‰ã€‚
```python
from torch.utils.data import DataLoader

def collate_fn(batch):
    # è¿‡æ»¤æ‰ None
    batch = [item for item in batch if item is not None]
    
    # å¦‚æœ batch ä¸ºç©ºï¼Œè¿”å› Noneï¼Œé¿å… torch.stack å‡ºé”™
    if len(batch) == 0:
        return None

    data = torch.stack([item[0] for item in batch])
    target = torch.stack([item[1] for item in batch])
    return data, target

train_dataloader = DataLoader(train_dataset, collate_fn=collate_fn, batch_size=2, shuffle=True)
batch = next(iter(train_dataloader))

# éªŒè¯åˆå§‹æŸå¤±
outputs = model(pixel_values=batch[0].to(device), labels=batch[1].to(device))
outputs.loss
```

æ˜¯æ—¶å€™è®­ç»ƒæ¨¡å‹äº†ï¼æˆ‘ä»¬å°†åœ¨æ­¤å¤„ä»¥å¸¸è§„çš„ PyTorch æ–¹å¼è¿›è¡Œè®­ç»ƒï¼Œä½†è¯·éšæ—¶å‡çº§ä»¥åˆ©ç”¨ ğŸ¤— Accelerateï¼ˆå¯¹äºå…·æœ‰æœ€å°‘ä»£ç æ›´æ”¹çš„åˆ†å¸ƒå¼è®­ç»ƒéå¸¸æœ‰ç”¨ï¼‰ï¼Œæˆ–è€…åˆ©ç”¨ ğŸ¤— Trainer ç±»æ¥å¤„ç†
æˆ‘ä»¬åœ¨æ­¤å¤„ä¸ºæ‚¨å®šä¹‰çš„è®¸å¤šé€»è¾‘ï¼ˆä¾‹å¦‚åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼‰ã€‚
- learning rate  å­¦ä¹ ç‡
- number of epochs  çºªå…ƒæ•°
- optimizer  ä¼˜åŒ–
- gradient accumulation, gradient checkpointing, Flash Attention can be leveraged to speed up training å¯ä»¥åˆ©ç”¨æ¢¯åº¦ç´¯ç§¯ã€æ¢¯åº¦æ£€æŸ¥ç‚¹ã€Flash Attention æ¥åŠ é€Ÿè®­ç»ƒ
- mixed precision training (bfloat16) etc. æ··åˆç²¾åº¦è®­ç»ƒ ï¼ˆbfloat16ï¼‰ ç­‰ã€‚
```python
class AverageMeter(object):
    """Computes and stores the average and current value"""
    def __init__(self):
        self.reset()

    def reset(self):
        self.val = 0
        self.avg = 0
        self.sum = 0
        self.count = 0

    def update(self, val, n=1):
        self.val = val
        self.sum += val * n
        self.count += n
        self.avg = self.sum / self.count

from torch.optim import AdamW
from tqdm.auto import tqdm

optimizer = AdamW(model.parameters(), lr=5e-5)

losses = AverageMeter()

model.train()
for epoch in range(10):  # loop over the dataset multiple times
    for idx, batch in enumerate(tqdm(train_dataloader)):
        # è·³è¿‡æ— æ•ˆæ‰¹æ¬¡
        if batch is None:
            continue
        # get the inputs;
        pixel_values, labels = batch

        # zero the parameter gradients
        optimizer.zero_grad()

        # forward pass
        outputs = model(
            pixel_values=pixel_values.to(device),
            labels=labels.to(device),
        )

        # calculate gradients
        loss = outputs.loss
        losses.update(loss.item(), pixel_values.size(0))
        loss.backward()

        # optimization step
        optimizer.step()

        if idx % 2000 == 0:
            print('Epoch: [{0}]\t'
                  'Loss {loss.val:.4f} ({loss.avg:.4f})\t'.format(
                   epoch, loss=losses,))
```

æµ‹è¯•è¿›è¡Œæ¨ç†
```python
image = Image.open("/root/autodl-tmp/multilabel_modified/images/image6179.jpg")
model.eval()

# prepare image for the model
pixel_values = processor(image, return_tensors="pt").pixel_values.to(device)

# forward pass
with torch.no_grad():
  outputs = model(pixel_values)
  logits = outputs.logits

# ç”±äºæˆ‘ä»¬åœ¨è®­ç»ƒæœŸé—´ä½¿ç”¨äº† BCEWithLogitsLossï¼ˆåœ¨è®¡ç®—æŸå¤±ä¹‹å‰å¯¹ logit åº”ç”¨ sigmoidï¼‰ï¼Œå› æ­¤æˆ‘ä»¬ä¹Ÿéœ€è¦åœ¨æ­¤å¤„å°† sigmoid åº”ç”¨äº logitsã€‚è¿™å°†å®ƒä»¬è½¬åŒ–ä¸ºå•ç‹¬çš„æ¦‚ç‡ã€‚
sigmoid = torch.nn.Sigmoid()
probs = sigmoid(logits.squeeze().cpu())

# select the probabilities > a certain threshold (e.g. 50%) as predicted
predictions = np.zeros(probs.shape)
predictions[np.where(probs >= 0.5)] = 1 # turn predicted id's into actual label names
predicted_labels = [id2label[idx] for idx, label in enumerate(predictions) if label == 1.0]
print(predicted_labels)
```

æŒä¹…åŒ–ä¿å­˜æ¨¡å‹
```python
model.save_pretrained("./saved_model/")
```

## å››ã€ç‰¹å¾æå–

æœ¬ä»“åº“ä¸­æ–°å¢äº† `siglip_vision_extractor.py` æ–‡ä»¶ï¼Œè¯¥è„šæœ¬ç”¨äºç‹¬ç«‹æå–å›¾ç‰‡/è§†é¢‘å¸§çš„è§†è§‰ç‰¹å¾ï¼Œ
å¹¶è¿›ä¸€æ­¥å±•ç¤ºå¦‚ä½•å°†è¿™äº›ç‰¹å¾ä¼ å…¥å…¶ä»–å¤§è¯­è¨€æ¨¡å‹ï¼ˆä¾‹å¦‚åŸºäº langchain å®ç°çš„ qwen æˆ– deepseekï¼‰ã€‚

### åŠŸèƒ½æ¦‚è¿°

- **è§†è§‰ç‰¹å¾æå–**: åˆ©ç”¨é¢„è®­ç»ƒçš„ Siglip æ¨¡å‹å¯¹å›¾åƒè¿›è¡Œç‰¹å¾æå–ï¼Œå…¶è¾“å‡ºå¼ é‡å½¢çŠ¶ä¸º (B, T, W, H, hidden_size)ã€‚
- **ç‰¹å¾å›¾å¯è§†åŒ–**: è„šæœ¬ä¸­ä½¿ç”¨ matplotlib å°†ç‰¹å¾éšè—ç»´åº¦çš„å¹³å‡å€¼å±•ç¤ºä¸ºçƒ­åŠ›å›¾ã€‚
- **ä¸ LLM ç»“åˆ**: æä¾›ç¤ºä¾‹å‡½æ•° `send_features_to_llm` å±•ç¤ºå¦‚ä½•å°†æå–çš„è§†è§‰ç‰¹å¾è½¬æ¢ä¸ºæç¤ºä¿¡æ¯ä¼ å…¥å¤§è¯­è¨€æ¨¡å‹ã€‚

### ä½¿ç”¨æ–¹æ³•

1. **ä¾èµ–å®‰è£…**: ç¡®ä¿å®‰è£…ä»¥ä¸‹ä¾èµ–ï¼š
   - torch
   - transformers
   - pillow (PIL)
   - opencv-python
   - matplotlib (å¯é€‰ï¼Œç”¨äºå¯è§†åŒ–)

2. **é›†æˆä½¿ç”¨**:
   `siglip_vision_extractor.py` æ–‡ä»¶å¹¶éè®¾è®¡ä¸ºç‹¬ç«‹è¿è¡Œçš„è„šæœ¬ï¼Œè€Œæ˜¯æä¾›äº†ä¸€ä¸ªç”¨äºæå–è§†è§‰ç‰¹å¾çš„ç±» `SiglipVisionTower`ï¼Œ
   ä¾›ç”¨æˆ·åœ¨å¤šæ¨¡æ€åº”ç”¨ä¸­è°ƒç”¨ã€‚ä½ å¯ä»¥é€šè¿‡ç›´æ¥å¯¼å…¥æ¨¡å—å¹¶å®ä¾‹åŒ–è¯¥ç±»æ¥é›†æˆä½¿ç”¨ï¼š

   ```python
   from siglip.siglip_vision_extractor import SiglipVisionTower, VisionTowerConfig

   # è®¾ç½®æ¨¡å‹è·¯å¾„ä¸é…ç½®å‚æ•°ï¼ˆè¯·æ›¿æ¢ä¸ºå®é™…æ¨¡å‹åç§°æˆ–è·¯å¾„ï¼‰
   model_name_or_path = "siglip-base"
   config = VisionTowerConfig(
       vision_tower_name=model_name_or_path,
       img_size=224,       
       patch_size=16,      
       hidden_size=768,    
       num_frames=1        
   )

   # åˆå§‹åŒ– SiglipVisionTower æ¨¡å‹
   model = SiglipVisionTower(model_name_or_path, config)

   # åŠ è½½å›¾åƒå¹¶æå–è§†è§‰ç‰¹å¾
   from PIL import Image
   img = Image.open("example.jpg").convert("RGB")  # è¯·ç¡®ä¿å­˜åœ¨ç¤ºä¾‹å›¾åƒ
   inputs = model.vision_processor(img, return_tensors="pt")
   features = model(inputs["pixel_values"])
   print("Extracted features shape:", features.shape)
   ```

   ä½ å¯ä»¥åœ¨æ­¤åŸºç¡€ä¸Šè¿›ä¸€æ­¥è®¾è®¡é™ç»´æˆ–çº¿æ€§æ˜ å°„æ¨¡å—ï¼Œå°†æå–çš„é«˜ç»´è§†è§‰ç‰¹å¾è½¬æ¢æˆé€‚åˆå¤§è¯­è¨€æ¨¡å‹çš„è¾“å…¥æ ¼å¼ï¼Œå¹¶ç»“åˆå…¶ä»– LLMï¼ˆä¾‹å¦‚åŸºäº langchain çš„ qwen æˆ– deepseekï¼‰ä½¿ç”¨ã€‚
