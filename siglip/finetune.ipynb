{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94e22c0e-a5a2-4265-a6b3-dc8595712b4b",
   "metadata": {},
   "source": [
    "接下来，将下载的文件中的 csv 读取为 Pandas 数据帧。每行都包含一个训练示例，其中包含图像的文件名和相应的 one-hot 编码标签。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fea8dc0e-f9de-4406-aae6-9be96aaa58af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image_Name</th>\n",
       "      <th>Classes(motorcycle, truck, boat, bus, cycle, person, desert, mountains, sea, sunset, trees, sitar, ektara, flutes, tabla, harmonium)</th>\n",
       "      <th>motorcycle</th>\n",
       "      <th>truck</th>\n",
       "      <th>boat</th>\n",
       "      <th>bus</th>\n",
       "      <th>cycle</th>\n",
       "      <th>person</th>\n",
       "      <th>desert</th>\n",
       "      <th>mountains</th>\n",
       "      <th>sea</th>\n",
       "      <th>sunset</th>\n",
       "      <th>trees</th>\n",
       "      <th>sitar</th>\n",
       "      <th>ektara</th>\n",
       "      <th>flutes</th>\n",
       "      <th>tabla</th>\n",
       "      <th>harmonium</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>image1.jpg</td>\n",
       "      <td>bus person</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>image2.jpg</td>\n",
       "      <td>sitar</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>image3.jpg</td>\n",
       "      <td>flutes</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>image4.jpg</td>\n",
       "      <td>bus trees</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>image5.jpg</td>\n",
       "      <td>bus</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Image_Name  \\\n",
       "0  image1.jpg   \n",
       "1  image2.jpg   \n",
       "2  image3.jpg   \n",
       "3  image4.jpg   \n",
       "4  image5.jpg   \n",
       "\n",
       "   Classes(motorcycle, truck, boat, bus, cycle, person, desert, mountains, sea, sunset, trees, sitar, ektara, flutes, tabla, harmonium)  \\\n",
       "0                                         bus person                                                                                      \n",
       "1                                              sitar                                                                                      \n",
       "2                                             flutes                                                                                      \n",
       "3                                          bus trees                                                                                      \n",
       "4                                                bus                                                                                      \n",
       "\n",
       "   motorcycle  truck  boat  bus  cycle  person  desert  mountains  sea  \\\n",
       "0           0      0     0    1      0       1       0          0    0   \n",
       "1           0      0     0    0      0       0       0          0    0   \n",
       "2           0      0     0    0      0       0       0          0    0   \n",
       "3           0      0     0    1      0       0       0          0    0   \n",
       "4           0      0     0    1      0       0       0          0    0   \n",
       "\n",
       "   sunset  trees  sitar  ektara  flutes  tabla  harmonium  \n",
       "0       0      0      0       0       0      0          0  \n",
       "1       0      0      1       0       0      0          0  \n",
       "2       0      0      0       0       1      0          0  \n",
       "3       0      1      0       0       0      0          0  \n",
       "4       0      0      0       0       0      0          0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/root/autodl-tmp/multilabel_modified/multilabel_classification(2).csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb45c303-1d02-4e28-bce3-10d8e6c4c1bf",
   "metadata": {},
   "source": [
    "创建一个 id2label 字典，将整数映射到字符串。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "311dcfe3-4937-4205-bc6f-29ab3882c6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'motorcycle', 1: 'truck', 2: 'boat', 3: 'bus', 4: 'cycle', 5: 'person', 6: 'desert', 7: 'mountains', 8: 'sea', 9: 'sunset', 10: 'trees', 11: 'sitar', 12: 'ektara', 13: 'flutes', 14: 'tabla', 15: 'harmonium'}\n"
     ]
    }
   ],
   "source": [
    "labels = list(df.columns)[2:]\n",
    "id2label = {id: label for id, label in enumerate(labels)}\n",
    "print(id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419970c3-1fa1-47c9-8ed8-225983b994f2",
   "metadata": {},
   "source": [
    "接下来加载离线模型与图像处理器，其中将problem_type指定为 “multi_label_classification”,其是告诉模型当前为多标签分类，从而促使其使用正确的激活函数，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2653263e-2533-43a9-84dc-ee13475b1287",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SiglipForImageClassification were not initialized from the model checkpoint at /root/autodl-tmp/siglip-so400m-patch14-384 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_id = \"/root/autodl-tmp/siglip-so400m-patch14-384\"\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained(model_id, device=device)\n",
    "model = AutoModelForImageClassification.from_pretrained(model_id, problem_type=\"multi_label_classification\", id2label=id2label)\n",
    "model = model.to(device) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10805fd4-10d0-4219-aa86-7229428a603e",
   "metadata": {},
   "source": [
    "创建数据集读取类，从而确保能够正确的读取图片以及分类标签并转换为正确的格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a78f7d1-5e6a-4945-8c37-43f22f78757a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "class MultiLabelDataset(Dataset):\n",
    "  def __init__(self, root, df, transform):\n",
    "    self.root = root\n",
    "    self.df = df\n",
    "    self.transform = transform\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    item = self.df.iloc[idx]\n",
    "    # get image\n",
    "    image_path = os.path.join(self.root, item[\"Image_Name\"])\n",
    "\n",
    "    if not os.path.exists(image_path):\n",
    "        return None\n",
    "\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    # prepare image for the model\n",
    "    pixel_values = self.transform(image)\n",
    "\n",
    "    # get labels\n",
    "    labels = item[2:].values.astype(np.float32)\n",
    "\n",
    "    # turn into PyTorch tensor\n",
    "    labels = torch.from_numpy(labels)\n",
    "\n",
    "    return pixel_values, labels\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6346f950-dae2-45d2-b1f3-a849a3cdc1a5",
   "metadata": {},
   "source": [
    "为了准备的图像，将使用 Torchvision 包，它提供了若干图像转换工具将图像大小调整为模型预期的大小（在本例中为 384），\n",
    "并且使用适当的平均值和标准偏差对颜色通道进行标准化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b5b860d-14de-42ac-bb00-5320eae82d57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8968"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "\n",
    "# get appropriate size, mean and std based on the image processor\n",
    "size = processor.size[\"height\"]\n",
    "mean = processor.image_mean\n",
    "std = processor.image_std\n",
    "\n",
    "transform = Compose([\n",
    "    Resize((size, size)),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=mean, std=std),\n",
    "])\n",
    "\n",
    "train_dataset = MultiLabelDataset(root=\"/root/autodl-tmp/multilabel_modified/images\",\n",
    "                                  df=df, transform=transform)\n",
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eda5f67-6f71-43e2-be66-679d4c024988",
   "metadata": {},
   "source": [
    "接下来，我们可以创建相应的 PyTorch DataLoader，以获取批量训练示例（因为神经网络通常使用随机梯度下降 = SGD 对批量数据进行训练）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a3ab3cfa-5454-47ea-b257-f0d823832938",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # 过滤掉 None\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    \n",
    "    # 如果 batch 为空，返回 None，避免 torch.stack 出错\n",
    "    if len(batch) == 0:\n",
    "        return None\n",
    "\n",
    "    data = torch.stack([item[0] for item in batch])\n",
    "    target = torch.stack([item[1] for item in batch])\n",
    "    return data, target\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, collate_fn=collate_fn, batch_size=2, shuffle=True)\n",
    "batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d05c81-59ff-46c6-bb60-43b9e9b3f77c",
   "metadata": {},
   "source": [
    "验证初始损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0fedfacd-90f1-4479-91d8-4a05272124b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0682, device='cuda:0',\n",
       "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(pixel_values=batch[0].to(device), labels=batch[1].to(device))\n",
    "outputs.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349b5f38-6c47-4b89-8238-9111d67f5f9c",
   "metadata": {},
   "source": [
    "是时候训练模型了！我们将在此处以常规的 PyTorch 方式进行训练，但请随时升级以利用 🤗 Accelerate（对于具有最少代码更改的分布式训练非常有用），或者利用 🤗 Trainer 类来处理\n",
    "我们在此处为您定义的许多逻辑（例如创建数据加载器）。\n",
    "- learning rate  学习率\n",
    "- number of epochs  纪元数\n",
    "- optimizer  优化\n",
    "- gradient accumulation, gradient checkpointing, Flash Attention can be leveraged to speed up training 可以利用梯度累积、梯度检查点、Flash Attention 来加速训练\n",
    "- mixed precision training (bfloat16) etc. 混合精度训练 （bfloat16） 等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc58dbb-5a9b-4d07-99e1-eeaf5c623bd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20e9587eacbf45c898d0c12077098fa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4484 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]\tLoss 0.1064 (0.1064)\t\n",
      "Epoch: [0]\tLoss 0.0090 (0.0550)\t\n",
      "Epoch: [0]\tLoss 0.0169 (0.0626)\t\n",
      "Epoch: [0]\tLoss 0.0126 (0.0621)\t\n",
      "Epoch: [0]\tLoss 0.0663 (0.0611)\t\n",
      "Epoch: [0]\tLoss 0.0018 (0.0600)\t\n",
      "Epoch: [0]\tLoss 0.0054 (0.0596)\t\n",
      "Epoch: [0]\tLoss 0.0179 (0.0600)\t\n",
      "Epoch: [0]\tLoss 0.0215 (0.0582)\t\n",
      "Epoch: [0]\tLoss 0.0723 (0.0583)\t\n",
      "Epoch: [0]\tLoss 0.0682 (0.0591)\t\n",
      "Epoch: [0]\tLoss 0.0049 (0.0585)\t\n",
      "Epoch: [0]\tLoss 0.1924 (0.0588)\t\n",
      "Epoch: [0]\tLoss 0.0254 (0.0583)\t\n",
      "Epoch: [0]\tLoss 0.0205 (0.0587)\t\n",
      "Epoch: [0]\tLoss 0.0268 (0.0595)\t\n",
      "Epoch: [0]\tLoss 0.1002 (0.0591)\t\n",
      "Epoch: [0]\tLoss 0.0089 (0.0584)\t\n",
      "Epoch: [0]\tLoss 0.0022 (0.0582)\t\n",
      "Epoch: [0]\tLoss 0.0266 (0.0574)\t\n",
      "Epoch: [0]\tLoss 0.0098 (0.0575)\t\n",
      "Epoch: [0]\tLoss 0.0328 (0.0577)\t\n",
      "Epoch: [0]\tLoss 0.0478 (0.0571)\t\n",
      "Epoch: [0]\tLoss 0.0020 (0.0570)\t\n",
      "Epoch: [0]\tLoss 0.0092 (0.0564)\t\n",
      "Epoch: [0]\tLoss 0.0048 (0.0575)\t\n",
      "Epoch: [0]\tLoss 0.0026 (0.0575)\t\n",
      "Epoch: [0]\tLoss 0.0026 (0.0580)\t\n",
      "Epoch: [0]\tLoss 0.0586 (0.0577)\t\n",
      "Epoch: [0]\tLoss 0.0716 (0.0578)\t\n",
      "Epoch: [0]\tLoss 0.1484 (0.0580)\t\n",
      "Epoch: [0]\tLoss 0.1403 (0.0582)\t\n",
      "Epoch: [0]\tLoss 0.0716 (0.0585)\t\n",
      "Epoch: [0]\tLoss 0.1542 (0.0583)\t\n",
      "Epoch: [0]\tLoss 0.0389 (0.0586)\t\n",
      "Epoch: [0]\tLoss 0.1383 (0.0589)\t\n",
      "Epoch: [0]\tLoss 0.0114 (0.0588)\t\n",
      "Epoch: [0]\tLoss 0.0050 (0.0589)\t\n",
      "Epoch: [0]\tLoss 0.0122 (0.0591)\t\n",
      "Epoch: [0]\tLoss 0.0101 (0.0590)\t\n",
      "Epoch: [0]\tLoss 0.0562 (0.0596)\t\n",
      "Epoch: [0]\tLoss 0.0100 (0.0599)\t\n",
      "Epoch: [0]\tLoss 0.0857 (0.0601)\t\n",
      "Epoch: [0]\tLoss 0.0110 (0.0602)\t\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "903bd1ad67644998a9965330d211e743",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4484 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1]\tLoss 0.0307 (0.0607)\t\n",
      "Epoch: [1]\tLoss 0.0455 (0.0603)\t\n",
      "Epoch: [1]\tLoss 0.0344 (0.0598)\t\n",
      "Epoch: [1]\tLoss 0.0298 (0.0598)\t\n",
      "Epoch: [1]\tLoss 0.0083 (0.0602)\t\n",
      "Epoch: [1]\tLoss 0.0147 (0.0600)\t\n",
      "Epoch: [1]\tLoss 0.0263 (0.0600)\t\n",
      "Epoch: [1]\tLoss 0.0060 (0.0596)\t\n",
      "Epoch: [1]\tLoss 0.0298 (0.0595)\t\n",
      "Epoch: [1]\tLoss 0.0026 (0.0595)\t\n",
      "Epoch: [1]\tLoss 0.0052 (0.0592)\t\n",
      "Epoch: [1]\tLoss 0.0013 (0.0588)\t\n",
      "Epoch: [1]\tLoss 0.0816 (0.0589)\t\n",
      "Epoch: [1]\tLoss 0.0055 (0.0589)\t\n",
      "Epoch: [1]\tLoss 0.0295 (0.0587)\t\n",
      "Epoch: [1]\tLoss 0.0732 (0.0588)\t\n",
      "Epoch: [1]\tLoss 0.0922 (0.0589)\t\n",
      "Epoch: [1]\tLoss 0.0009 (0.0588)\t\n",
      "Epoch: [1]\tLoss 0.0141 (0.0593)\t\n",
      "Epoch: [1]\tLoss 0.2931 (0.0592)\t\n",
      "Epoch: [1]\tLoss 0.0268 (0.0592)\t\n",
      "Epoch: [1]\tLoss 0.0310 (0.0592)\t\n",
      "Epoch: [1]\tLoss 0.0315 (0.0595)\t\n",
      "Epoch: [1]\tLoss 0.0953 (0.0594)\t\n",
      "Epoch: [1]\tLoss 0.0036 (0.0593)\t\n",
      "Epoch: [1]\tLoss 0.0080 (0.0594)\t\n",
      "Epoch: [1]\tLoss 0.1460 (0.0593)\t\n",
      "Epoch: [1]\tLoss 0.0919 (0.0592)\t\n",
      "Epoch: [1]\tLoss 0.0160 (0.0591)\t\n",
      "Epoch: [1]\tLoss 0.0735 (0.0589)\t\n",
      "Epoch: [1]\tLoss 0.1988 (0.0588)\t\n",
      "Epoch: [1]\tLoss 0.0381 (0.0591)\t\n",
      "Epoch: [1]\tLoss 0.0062 (0.0591)\t\n",
      "Epoch: [1]\tLoss 0.0007 (0.0591)\t\n",
      "Epoch: [1]\tLoss 0.0092 (0.0593)\t\n",
      "Epoch: [1]\tLoss 0.0046 (0.0592)\t\n",
      "Epoch: [1]\tLoss 0.1065 (0.0592)\t\n",
      "Epoch: [1]\tLoss 0.0014 (0.0593)\t\n",
      "Epoch: [1]\tLoss 0.0216 (0.0591)\t\n",
      "Epoch: [1]\tLoss 0.0181 (0.0592)\t\n",
      "Epoch: [1]\tLoss 0.0107 (0.0596)\t\n",
      "Epoch: [1]\tLoss 0.0017 (0.0595)\t\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cdcb1d116c04637a1ec824f380e8d00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4484 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2]\tLoss 0.0017 (0.0594)\t\n",
      "Epoch: [2]\tLoss 0.0542 (0.0592)\t\n",
      "Epoch: [2]\tLoss 0.0030 (0.0591)\t\n",
      "Epoch: [2]\tLoss 0.1584 (0.0590)\t\n",
      "Epoch: [2]\tLoss 0.0018 (0.0590)\t\n",
      "Epoch: [2]\tLoss 0.0417 (0.0590)\t\n",
      "Epoch: [2]\tLoss 0.0112 (0.0588)\t\n",
      "Epoch: [2]\tLoss 0.1308 (0.0587)\t\n",
      "Epoch: [2]\tLoss 0.0051 (0.0588)\t\n",
      "Epoch: [2]\tLoss 0.0810 (0.0587)\t\n",
      "Epoch: [2]\tLoss 0.0768 (0.0587)\t\n",
      "Epoch: [2]\tLoss 0.1360 (0.0589)\t\n",
      "Epoch: [2]\tLoss 0.0614 (0.0588)\t\n",
      "Epoch: [2]\tLoss 0.0005 (0.0587)\t\n",
      "Epoch: [2]\tLoss 0.0491 (0.0586)\t\n",
      "Epoch: [2]\tLoss 0.0357 (0.0585)\t\n",
      "Epoch: [2]\tLoss 0.4642 (0.0584)\t\n",
      "Epoch: [2]\tLoss 0.2217 (0.0583)\t\n",
      "Epoch: [2]\tLoss 0.0920 (0.0582)\t\n",
      "Epoch: [2]\tLoss 0.1208 (0.0581)\t\n",
      "Epoch: [2]\tLoss 0.0692 (0.0581)\t\n",
      "Epoch: [2]\tLoss 0.0492 (0.0582)\t\n",
      "Epoch: [2]\tLoss 0.0213 (0.0583)\t\n",
      "Epoch: [2]\tLoss 0.0018 (0.0582)\t\n",
      "Epoch: [2]\tLoss 0.2407 (0.0581)\t\n",
      "Epoch: [2]\tLoss 0.1662 (0.0581)\t\n",
      "Epoch: [2]\tLoss 0.0023 (0.0579)\t\n",
      "Epoch: [2]\tLoss 0.0109 (0.0581)\t\n",
      "Epoch: [2]\tLoss 0.0626 (0.0581)\t\n",
      "Epoch: [2]\tLoss 0.0066 (0.0581)\t\n",
      "Epoch: [2]\tLoss 0.0449 (0.0580)\t\n",
      "Epoch: [2]\tLoss 0.0056 (0.0581)\t\n",
      "Epoch: [2]\tLoss 0.5484 (0.0580)\t\n",
      "Epoch: [2]\tLoss 0.2406 (0.0581)\t\n",
      "Epoch: [2]\tLoss 0.2128 (0.0580)\t\n",
      "Epoch: [2]\tLoss 0.0074 (0.0579)\t\n",
      "Epoch: [2]\tLoss 0.0138 (0.0578)\t\n",
      "Epoch: [2]\tLoss 0.0053 (0.0577)\t\n",
      "Epoch: [2]\tLoss 0.0654 (0.0579)\t\n",
      "Epoch: [2]\tLoss 0.1381 (0.0580)\t\n",
      "Epoch: [2]\tLoss 0.0256 (0.0580)\t\n",
      "Epoch: [2]\tLoss 0.3539 (0.0579)\t\n",
      "Epoch: [2]\tLoss 0.0064 (0.0580)\t\n",
      "Epoch: [2]\tLoss 0.0809 (0.0579)\t\n",
      "Epoch: [2]\tLoss 0.0553 (0.0579)\t\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b385690b1c7145638bacb6ea669daa54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4484 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3]\tLoss 0.0385 (0.0579)\t\n",
      "Epoch: [3]\tLoss 0.3101 (0.0578)\t\n",
      "Epoch: [3]\tLoss 0.0100 (0.0577)\t\n",
      "Epoch: [3]\tLoss 0.0094 (0.0576)\t\n"
     ]
    }
   ],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "losses = AverageMeter()\n",
    "\n",
    "model.train()\n",
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "    for idx, batch in enumerate(tqdm(train_dataloader)):\n",
    "        # 跳过无效批次\n",
    "        if batch is None:\n",
    "            continue\n",
    "        # get the inputs;\n",
    "        pixel_values, labels = batch\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass\n",
    "        outputs = model(\n",
    "            pixel_values=pixel_values.to(device),\n",
    "            labels=labels.to(device),\n",
    "        )\n",
    "\n",
    "        # calculate gradients\n",
    "        loss = outputs.loss\n",
    "        losses.update(loss.item(), pixel_values.size(0))\n",
    "        loss.backward()\n",
    "\n",
    "        # optimization step\n",
    "        optimizer.step()\n",
    "\n",
    "        if idx % 2000 == 0:\n",
    "            print('Epoch: [{0}]\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(\n",
    "                   epoch, loss=losses,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe006689-3323-46e5-bcaa-004bdef2ac3b",
   "metadata": {},
   "source": [
    "测试进行推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a29fd2fb-287c-4a00-a47f-66fee693625d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bus', 'trees']\n"
     ]
    }
   ],
   "source": [
    "image = Image.open(\"/root/autodl-tmp/multilabel_modified/images/image6179.jpg\")\n",
    "model.eval()\n",
    "\n",
    "# prepare image for the model\n",
    "pixel_values = processor(image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "\n",
    "# forward pass\n",
    "with torch.no_grad():\n",
    "  outputs = model(pixel_values)\n",
    "  logits = outputs.logits\n",
    "\n",
    "# 由于我们在训练期间使用了 BCEWithLogitsLoss（在计算损失之前对 logit 应用 sigmoid），因此我们也需要在此处将 sigmoid 应用于 logits。这将它们转化为单独的概率。\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "probs = sigmoid(logits.squeeze().cpu())\n",
    "\n",
    "# select the probabilities > a certain threshold (e.g. 50%) as predicted\n",
    "predictions = np.zeros(probs.shape)\n",
    "predictions[np.where(probs >= 0.5)] = 1 # turn predicted id's into actual label names\n",
    "predicted_labels = [id2label[idx] for idx, label in enumerate(predictions) if label == 1.0]\n",
    "print(predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f3e07c-4fe6-4ca5-b831-1a269fbfcaa6",
   "metadata": {},
   "source": [
    "持久化保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f19d991-ad78-4a7f-a496-b95cca538aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./saved_model/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "siglip",
   "language": "python",
   "name": "siglip"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
